# ResNet (Residual Network) の概要

ResNeXtを学ぶ前に、その基盤となる**ResNet (Residual Network)**について理解することは非常に重要です。ResNetは、深層学習における画像認識の分野に革命をもたらした画期的なアーキテクチャであり、その後の多くのモデルに影響を与えました。

## 1. なぜResNetが必要だったのか？

ディープラーニングの世界では、「層を深くすればするほど、モデルの表現力が高まり、性能が向上する」という考え方が一般的でした。しかし、実際にネットワークの層を闇雲に深くしていくと、以下のような問題に直面しました。

* **勾配消失問題 (Vanishing Gradient Problem)**:
    ネットワークが深くなるにつれて、学習時に誤差逆伝播法（バックプロパゲーション）で計算される勾配が、前の層へ伝播するにつれて非常に小さくなってしまう現象です。これにより、初期の層の重みがほとんど更新されなくなり、学習が停滞してしまいます。
* **勾配爆発問題 (Exploding Gradient Problem)**:
    勾配消失とは逆に、勾配が非常に大きくなりすぎてしまい、重みが急激に更新され、学習が不安定になる現象です。
* **性能の劣化問題 (Degradation Problem)**:
    勾配消失・爆発とは異なり、層の数を増やしても**訓練誤差自体が減少しなくなる**現象です。これは、単に最適化が難しくなるだけでなく、深層モデルが浅いモデルよりも「表現できない」という、より根本的な問題として認識されました。理想的には、深いモデルは浅いモデルの機能を包含し、さらに良い性能を出すべきですが、実際にはそうなっていませんでした。

これらの問題は、単に「層を深くする」だけでは解決できない、深層ネットワーク固有の課題でした。

## 2. ResNetの核心：残差ブロック (Residual Block)

ResNetがこれらの問題を解決するために導入したのが、**残差ブロック (Residual Block)** と呼ばれる画期的な構造です。

残差ブロックは、層の入力 $x$ を、その層の出力に直接加算する**スキップコネクション（またはショートカットコネクション）**を持ちます。

### 通常のブロックと残差ブロックの比較

**通常のブロック:**

$$
H(x) = F(x)
$$

ここで $F(x)$ は、畳み込み層や活性化関数などの一連の操作を表します。

**残差ブロック:**

$$
H(x) = F(x) + x
$$

残差ブロックでは、学習すべき内容を $F(x)$ ではなく、入力 $x$ に対する**残差 (Residual)**、つまり $F(x) = H(x) - x$ となるようにします。

![Residual Block Diagram](https://raw.githubusercontent.com/your-repo/assets/residual_block.png)
*図：残差ブロックの概念図 (簡略化)*

### なぜ残差ブロックが有効なのか？

1.  **勾配消失問題の緩和**:
    スキップコネクションがあることで、勾配は常に直接次の層に流れるパス（ショートカットパス）を持つことになります。これにより、たとえ $F(x)$ を通るパスの勾配が小さくなったとしても、$x$ を通るパスによって勾配がゼロになることを防ぎ、深いネットワークでも効果的に勾配が伝播されるようになります。

2.  **恒等写像 (Identity Mapping) の学習の容易さ**:
    もし、ある層が何も情報を追加する必要がない（恒等写像 $H(x) = x$ で十分）と判断した場合、通常のネットワークでは $F(x) = x$ となるように複雑な非線形変換を学習する必要があります。しかし、残差ブロックでは、$F(x) = 0$ を学習するだけで恒等写像を実現できます。ゼロを学習する方が、複雑な非線形変換を学習するよりもはるかに容易です。これにより、ネットワークは必要に応じて「何もしない」ことを選択できるようになり、性能の劣化問題を回避できます。

3.  **情報伝達の効率化**:
    入力 $x$ が直接次の層に渡されることで、深い層にまで入力情報がスムーズに伝達されるため、情報の欠損が少なくなります。

## 3. ResNetの主要アーキテクチャ

ResNetには、層の数によってResNet-18, ResNet-34, ResNet-50, ResNet-101, ResNet-152といった様々なバリエーションがあります。ResNet-50以上では、より効率的な**ボトルネックブロック**と呼ばれる構造が用いられますが、基本的な考え方は残差ブロックと同様です。

ResNetは、そのシンプルさと効果から、深層学習における基礎的なアーキテクチャとして広く利用されています。次のセクションでは、このResNetのアイデアをさらに発展させたResNeXtについて見ていきましょう。