# ResNeXt の紹介

ResNetが深層ネットワークの学習を可能にした画期的なモデルであるのに対し、**ResNeXt**はResNetの成功を基盤とし、さらにその性能と効率を向上させたアーキテクチャです。ResNeXtは、シンプルな設計原則に基づきながら、ResNetよりも少ないパラメータで同等以上の精度を達成できることを示しました。

## 1. ResNeXtの核心：Aggregated Transformations (集約された変換)

ResNeXtの最も重要なアイデアは、「**Aggregated Transformations (集約された変換)**」です。これは、複数の並列パス（分岐）を持つ構造を導入し、それぞれのパスで異なる変換を行い、それらの結果を集約（結合）するというものです。

従来のResNetブロックでは、主に単一のパスで畳み込み操作を行っていました。しかし、ResNeXtでは、この単一のパスを複数の並列な「ブランチ」に置き換えます。

### 構造のイメージ

ResNeXtのブロックは、ResNetの残差ブロックを基にしていますが、$F(x)$ の部分が複数の並列な経路に置き換えられています。

$$
H(x) = \sum_{i=1}^{C} F_i(x) + x
$$

ここで、$C$ は**カーディナリティ (Cardinality)** と呼ばれるもので、並列するブランチの数を表します。各 $F_i(x)$ は、同じトポロジー（構造）を持つ独立した変換パスです。

![ResNeXt Block Diagram](https://raw.githubusercontent.com/your-repo/assets/resnext_block.png)
*図：ResNeXtブロックの概念図 (簡略化)*

この構造は、GoogleNet/Inceptionシリーズの「Inceptionモジュール」に似ていますが、ResNeXtは各ブランチが同じトポロジーを持つという点で異なります。これにより、設計の複雑さを増すことなく、より多くの並列計算を行うことが可能になります。

## 2. Cardinality (カーディナリティ) の重要性

**カーディナリティ (Cardinality)** は、ResNeXtの性能を決定する重要なハイパーパラメータであり、**集約される変換の数（つまり、並列するブランチの数）**を指します。

ResNeXtの論文では、ネットワークの深さや幅を増やすよりも、カーディナリティを増やす方が、より効果的にモデルの表現能力を向上させ、精度を高めることができると示されています。

* **深さ (Depth)**: 層の数を増やす
* **幅 (Width)**: 各層のチャネル数を増やす
* **カーディナリティ (Cardinality)**: 並列するブランチの数を増やす

ResNeXtの実験では、カーディナリティを増やすことが、パラメータ数を大幅に増やすことなく、より良い性能をもたらすことが示されました。これは、モデルがより多様な特徴表現を学習できるようになるためと考えられます。

### 具体的なブロック構造 (ボトルネックブロックの例)

ResNeXtの各ブランチは、通常、以下のようなボトルネック構造を持つ畳み込み層のシーケンスで構成されます。

1.  1x1 畳み込み (チャネル数を削減)
2.  3x3 畳み込み (特徴抽出、グループ化畳み込みを使用)
3.  1x1 畳み込み (チャネル数を復元)

この3x3畳み込みでは、特に**グループ化畳み込み (Grouped Convolution)**が利用されます。これは、入力チャネルを複数のグループに分割し、それぞれのグループ内で独立して畳み込みを行う手法です。カーディナリティ $C$ は、このグループの数に対応します。

## 3. ResNeXtの利点

* **高い精度**: ResNetと比較して、同等またはそれ以下のパラメータ数でより高い精度を達成できます。
* **効率的な計算**: グループ化畳み込みの利用により、並列処理が容易になり、GPUなどの並列計算アーキテクチャで効率的に実行できます。
* **シンプルな設計**: 各ブランチが同じトポロジーを持つため、ネットワークの設計が比較的シンプルに保たれます。

ResNeXtは、その後のEfficientNetなどのモデルにおける「複合スケーリング（深さ、幅、解像度、カーディナリティなどをバランス良くスケーリングする）」の考え方にも影響を与えました。

次のセクションでは、ResNetとResNeXtの理解度を確認するための簡単なクイズを行います。

---